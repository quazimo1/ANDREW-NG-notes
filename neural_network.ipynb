{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quazimo1/ANDREW-NG-notes/blob/main/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AxntrS6d064"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler    \n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "dataset=pd.read_csv('/content/drive/MyDrive/data_set/fashion_neural.csv')\n"
      ],
      "metadata": {
        "id": "uK-TrGyEeGKt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "470ae6ef-0ae9-4aaa-9a45-0570ebabfd31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dependent_variable='label'\n",
        "independent_variables=dataset.columns.tolist()\n",
        "independent_variables.remove(dependent_variable)\n",
        "X=dataset[independent_variables].values\n",
        "Y=dataset[dependent_variable].values\n",
        "X_train, X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=0)\n",
        "scaler=MinMaxScaler()\n",
        "X_train=scaler.fit_transform(X_train)\n",
        "X_test=scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "ILqTcuLeeYoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "\n",
        "trainset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(Y_train))\n",
        "valset = TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(Y_test))\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=False)\n",
        "valloader= DataLoader(valset, batch_size=batch_size, shuffle=False) \n"
      ],
      "metadata": {
        "id": "-Urf3mg7ftsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 784\n",
        "hidden_sizes = [200, 100]\n",
        "output_size = 10\n",
        "\n",
        "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[1], output_size),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "print(model)"
      ],
      "metadata": {
        "id": "XrPH3YxniQ5J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "237a04dd-c6af-4c90-e886-0ae801f1d060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=784, out_features=200, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=200, out_features=100, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=100, out_features=10, bias=True)\n",
            "  (5): LogSoftmax(dim=1)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.NLLLoss()\n",
        "images, labels = next(iter(trainloader))\n",
        "images = images.view(images.shape[0], -1)\n",
        "\n",
        "logps = model(images) #log probabilities\n",
        "loss = criterion(logps, labels) #calculate the NLL loss"
      ],
      "metadata": {
        "id": "J5Rmg3pniVur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
        "time0 = time()\n",
        "epochs = 20\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in trainloader:\n",
        "        \n",
        "        images = images.view(images.shape[0], -1)\n",
        "    \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "      \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "    else:\n",
        "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\n",
        "print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8MS8CrKijYV",
        "outputId": "81c718c2-7726-44f4-9981-d3cd7d089710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Training loss: 2.234891253709793\n",
            "Epoch 1 - Training loss: 1.828180728852749\n",
            "Epoch 2 - Training loss: 1.1696679323911667\n",
            "Epoch 3 - Training loss: 0.9210743129253387\n",
            "Epoch 4 - Training loss: 0.817867012321949\n",
            "Epoch 5 - Training loss: 0.7521111972630024\n",
            "Epoch 6 - Training loss: 0.7036901220679284\n",
            "Epoch 7 - Training loss: 0.665532848611474\n",
            "Epoch 8 - Training loss: 0.634427922219038\n",
            "Epoch 9 - Training loss: 0.6085112553089858\n",
            "Epoch 10 - Training loss: 0.5864146798849106\n",
            "Epoch 11 - Training loss: 0.5672990422695875\n",
            "Epoch 12 - Training loss: 0.5504825465381146\n",
            "Epoch 13 - Training loss: 0.5355267733335495\n",
            "Epoch 14 - Training loss: 0.5222433757036924\n",
            "Epoch 15 - Training loss: 0.5103865262120962\n",
            "Epoch 16 - Training loss: 0.4997402735054493\n",
            "Epoch 17 - Training loss: 0.49007458575069907\n",
            "Epoch 18 - Training loss: 0.4812259439378977\n",
            "Epoch 19 - Training loss: 0.4730673283338547\n",
            "\n",
            "Training Time (in minutes) = 0.0731657346089681\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct_count, all_count = 0, 0\n",
        "for images,labels in valloader:\n",
        "  for i in range(len(labels)):\n",
        "    img = images[i].view(1, 784)\n",
        "    with torch.no_grad():\n",
        "        logps = model(img)\n",
        "\n",
        "    \n",
        "    ps = torch.exp(logps)\n",
        "    probab = list(ps.numpy()[0])\n",
        "    pred_label = probab.index(max(probab))\n",
        "    true_label = labels.numpy()[i]\n",
        "    if(true_label == pred_label):\n",
        "      correct_count += 1\n",
        "    all_count += 1\n",
        "\n",
        "print(\"Accuracy % =\", ((correct_count/all_count)*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkZf_Zl5ixQZ",
        "outputId": "c598e36b-b9c3-4838-855b-567b87593beb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy % = 82.19999999999999\n"
          ]
        }
      ]
    }
  ]
}